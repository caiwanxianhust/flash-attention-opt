本文针对 Flash Attention 提出背景和计算原理进行了介绍，针对开源项目 Flash Attention Minimal 实现思路进行了解析和点评，并逐步优化了 Flash Attention 算子，给出了 3 个版本的 CUDA Kernel 并对实现思路进行详细介绍。

在 NVIDIA RTX 4060 上的测试数据（单位：ms），注意表中的 v1、v2、v3 并不是 Flash Attention 源项目的版本，而是笔者自己实现的 CUDA Kernel 的优化版本，不能混为一谈。

|`[bs, nh, N, M, d]`|Baseline|Minimal|v1|v2|v3|
|:---:|:---:|:---:|:---:|:---:|:---:|
|[32, 8, 256, 256, 256]|46.7941|741.067|104.727|127.366|12.4245|
|[32, 8, 256, 256, 1024]|56.871|9544.46|618.695|542.985|48.8425|
|[32, 8, 1024, 1024, 256]|92.6972|11343.9|1524.32|2026.06|167.241|
|[32, 8, 1024, 1024, 1024]|232.265|153121|10134.3|8636.17|707.792|

可见，整体来看：
- 序列长度较小时，v3 Kernel 性能是最好的，当序列长度扩大后，逐渐落后于 Baseline；
- v3 相比于 Flash Attention Minimal 性能提升巨大，且随着数据量的扩大甚至会有数百倍的差距。
- 当然，Baseline 性能比较好的原因，主要在于 cuBLAS 库底层针对不同 GEMM 尺寸会有自适应匹配最优 Kernel 机制，总能取得近乎最优的性能，这一点自定义 Kernel 很难超越，更不用说还没经过参数调优；
- v3 版本其实还有不少可以优化的点，比如 gemm 计算优化，如，加入双缓冲机制，load_matrix_sync 时进一步优化 bank conflict。再比如，进一步优化计算思路，Flash Attention 原作者其实更新了不少版本，从计算原理上有所优化，笔者还没有时间进一步阅读新的版本。
- 虽然 Baseline 性能比较好，但是其中间变量所占的全局内存（即显存）也不可忽视，相比于 Flash Attention 几乎多了 `bs * nh * N * M * 2 * sizeof(float)` 大小，在 `[32, 8, 1024, 1024, 1024]` 尺寸场景下，显存多占用了 2 GB，相当于 Flash Attention 的 50%，且 Flash Attention 的显存占用不受序列长度制约，这一点是非常重要的。